{"cells":[{"cell_type":"code","source":["import nltk \nnltk.download('punkt')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\nOut[1]: True</div>"]}}],"execution_count":1},{"cell_type":"code","source":["text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\nmy_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas  as well?\"\nprint(word_tokenize(my_text))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Hi&#39;, &#39;Mr.&#39;, &#39;Smith&#39;, &#39;!&#39;, &#39;I&#39;, &#39;’&#39;, &#39;m&#39;, &#39;going&#39;, &#39;to&#39;, &#39;buy&#39;, &#39;some&#39;, &#39;vegetables&#39;, &#39;(&#39;, &#39;tomatoes&#39;, &#39;and&#39;, &#39;cucumbers&#39;, &#39;)&#39;, &#39;from&#39;, &#39;the&#39;, &#39;store&#39;, &#39;.&#39;, &#39;Should&#39;, &#39;I&#39;, &#39;pick&#39;, &#39;up&#39;, &#39;some&#39;, &#39;black-eyed&#39;, &#39;peas&#39;, &#39;as&#39;, &#39;well&#39;, &#39;?&#39;]\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\nprint(sent_tokenize(my_text))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Hi Mr. Smith!&#39;, &#39;I’m going to buy some vegetables (tomatoes and cucumbers) from the store.&#39;, &#39;Should I pick up some black-eyed peas  as well?&#39;]\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["from nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nword_tok = word_tokenize(my_text)\ntwograms = list(ngrams(word_tok,2))\nprint(twograms)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;Hi&#39;, &#39;Mr.&#39;), (&#39;Mr.&#39;, &#39;Smith&#39;), (&#39;Smith&#39;, &#39;!&#39;), (&#39;!&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;’&#39;), (&#39;’&#39;, &#39;m&#39;), (&#39;m&#39;, &#39;going&#39;), (&#39;going&#39;, &#39;to&#39;), (&#39;to&#39;, &#39;buy&#39;), (&#39;buy&#39;, &#39;some&#39;), (&#39;some&#39;, &#39;vegetables&#39;), (&#39;vegetables&#39;, &#39;(&#39;), (&#39;(&#39;, &#39;tomatoes&#39;), (&#39;tomatoes&#39;, &#39;and&#39;), (&#39;and&#39;, &#39;cucumbers&#39;), (&#39;cucumbers&#39;, &#39;)&#39;), (&#39;)&#39;, &#39;from&#39;), (&#39;from&#39;, &#39;the&#39;), (&#39;the&#39;, &#39;store&#39;), (&#39;store&#39;, &#39;.&#39;), (&#39;.&#39;, &#39;Should&#39;), (&#39;Should&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;pick&#39;), (&#39;pick&#39;, &#39;up&#39;), (&#39;up&#39;, &#39;some&#39;), (&#39;some&#39;, &#39;black-eyed&#39;), (&#39;black-eyed&#39;, &#39;peas&#39;), (&#39;peas&#39;, &#39;as&#39;), (&#39;as&#39;, &#39;well&#39;), (&#39;well&#39;, &#39;?&#39;)]\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Tokenization(Regular Expression)"],"metadata":{}},{"cell_type":"code","source":["from nltk.tokenize import RegexpTokenizer\nwhitespace_tokenizer = RegexpTokenizer(\"\\s+\",gaps = True)\n\nprint(whitespace_tokenizer.tokenize(my_text))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Hi&#39;, &#39;Mr.&#39;, &#39;Smith!&#39;, &#39;I’m&#39;, &#39;going&#39;, &#39;to&#39;, &#39;buy&#39;, &#39;some&#39;, &#39;vegetables&#39;, &#39;(tomatoes&#39;, &#39;and&#39;, &#39;cucumbers)&#39;, &#39;from&#39;, &#39;the&#39;, &#39;store.&#39;, &#39;Should&#39;, &#39;I&#39;, &#39;pick&#39;, &#39;up&#39;, &#39;some&#39;, &#39;black-eyed&#39;, &#39;peas&#39;, &#39;as&#39;, &#39;well?&#39;]\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["from nltk.tokenize import RegexpTokenizer\ncap_Tokenizer = RegexpTokenizer(\"[A-Z]['\\w']+\")\nprint(cap_Tokenizer.tokenize(my_text))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Hi&#39;, &#39;Mr&#39;, &#39;Smith&#39;, &#39;Should&#39;]\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## Remove Punctuation"],"metadata":{}},{"cell_type":"code","source":["my_text = \"'Hi Mr Smith I m going to buy some vegetables tomatoes and cucumbers from the store Should I pick up 2lbs of black eyed peas as well\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["import re\nimport string\n# Replace punctuations with a white space\nclean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\nclean_text"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: &#39; Hi Mr Smith I m going to buy some vegetables tomatoes and cucumbers from the store Should I pick up 2lbs of black eyed peas as well&#39;</div>"]}}],"execution_count":11},{"cell_type":"code","source":["clean_text = clean_text.lower()\nprint(clean_text)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"> hi mr smith i m going to buy some vegetables tomatoes and cucumbers from the store should i pick up 2lbs of black eyed peas as well\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\nclean_text"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: &#39; hi mr smith i m going to buy some vegetables tomatoes and cucumbers from the store should i pick up   of black eyed peas as well&#39;</div>"]}}],"execution_count":13},{"cell_type":"code","source":["square_me = lambda x: x*x\nprint(square_me(5))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">25\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["text1 = \"I'm going to buy 5 cans of beans\"\ntext2 = \"I'm going to buy 5 can of ham\"\ntext3 = \"I'm going to 111 Dore Street\"\n\ntext = [text1,text2,text3]\n\nremove_number = lambda x :re.sub('\\w*\\d\\w*', ' ', x)\nprint(list(map(remove_number,text)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#34;I&#39;m going to buy   cans of beans&#34;, &#34;I&#39;m going to buy   can of ham&#34;, &#34;I&#39;m going to   Dore Street&#34;]\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["## Preprocessing: Stop Words"],"metadata":{}},{"cell_type":"code","source":["import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nset(stopwords.words('english'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\nOut[14]: {&#39;a&#39;,\n &#39;about&#39;,\n &#39;above&#39;,\n &#39;after&#39;,\n &#39;again&#39;,\n &#39;against&#39;,\n &#39;ain&#39;,\n &#39;all&#39;,\n &#39;am&#39;,\n &#39;an&#39;,\n &#39;and&#39;,\n &#39;any&#39;,\n &#39;are&#39;,\n &#39;aren&#39;,\n &#34;aren&#39;t&#34;,\n &#39;as&#39;,\n &#39;at&#39;,\n &#39;be&#39;,\n &#39;because&#39;,\n &#39;been&#39;,\n &#39;before&#39;,\n &#39;being&#39;,\n &#39;below&#39;,\n &#39;between&#39;,\n &#39;both&#39;,\n &#39;but&#39;,\n &#39;by&#39;,\n &#39;can&#39;,\n &#39;couldn&#39;,\n &#34;couldn&#39;t&#34;,\n &#39;d&#39;,\n &#39;did&#39;,\n &#39;didn&#39;,\n &#34;didn&#39;t&#34;,\n &#39;do&#39;,\n &#39;does&#39;,\n &#39;doesn&#39;,\n &#34;doesn&#39;t&#34;,\n &#39;doing&#39;,\n &#39;don&#39;,\n &#34;don&#39;t&#34;,\n &#39;down&#39;,\n &#39;during&#39;,\n &#39;each&#39;,\n &#39;few&#39;,\n &#39;for&#39;,\n &#39;from&#39;,\n &#39;further&#39;,\n &#39;had&#39;,\n &#39;hadn&#39;,\n &#34;hadn&#39;t&#34;,\n &#39;has&#39;,\n &#39;hasn&#39;,\n &#34;hasn&#39;t&#34;,\n &#39;have&#39;,\n &#39;haven&#39;,\n &#34;haven&#39;t&#34;,\n &#39;having&#39;,\n &#39;he&#39;,\n &#39;her&#39;,\n &#39;here&#39;,\n &#39;hers&#39;,\n &#39;herself&#39;,\n &#39;him&#39;,\n &#39;himself&#39;,\n &#39;his&#39;,\n &#39;how&#39;,\n &#39;i&#39;,\n &#39;if&#39;,\n &#39;in&#39;,\n &#39;into&#39;,\n &#39;is&#39;,\n &#39;isn&#39;,\n &#34;isn&#39;t&#34;,\n &#39;it&#39;,\n &#34;it&#39;s&#34;,\n &#39;its&#39;,\n &#39;itself&#39;,\n &#39;just&#39;,\n &#39;ll&#39;,\n &#39;m&#39;,\n &#39;ma&#39;,\n &#39;me&#39;,\n &#39;mightn&#39;,\n &#34;mightn&#39;t&#34;,\n &#39;more&#39;,\n &#39;most&#39;,\n &#39;mustn&#39;,\n &#34;mustn&#39;t&#34;,\n &#39;my&#39;,\n &#39;myself&#39;,\n &#39;needn&#39;,\n &#34;needn&#39;t&#34;,\n &#39;no&#39;,\n &#39;nor&#39;,\n &#39;not&#39;,\n &#39;now&#39;,\n &#39;o&#39;,\n &#39;of&#39;,\n &#39;off&#39;,\n &#39;on&#39;,\n &#39;once&#39;,\n &#39;only&#39;,\n &#39;or&#39;,\n &#39;other&#39;,\n &#39;our&#39;,\n &#39;ours&#39;,\n &#39;ourselves&#39;,\n &#39;out&#39;,\n &#39;over&#39;,\n &#39;own&#39;,\n &#39;re&#39;,\n &#39;s&#39;,\n &#39;same&#39;,\n &#39;shan&#39;,\n &#34;shan&#39;t&#34;,\n &#39;she&#39;,\n &#34;she&#39;s&#34;,\n &#39;should&#39;,\n &#34;should&#39;ve&#34;,\n &#39;shouldn&#39;,\n &#34;shouldn&#39;t&#34;,\n &#39;so&#39;,\n &#39;some&#39;,\n &#39;such&#39;,\n &#39;t&#39;,\n &#39;than&#39;,\n &#39;that&#39;,\n &#34;that&#39;ll&#34;,\n &#39;the&#39;,\n &#39;their&#39;,\n &#39;theirs&#39;,\n &#39;them&#39;,\n &#39;themselves&#39;,\n &#39;then&#39;,\n &#39;there&#39;,\n &#39;these&#39;,\n &#39;they&#39;,\n &#39;this&#39;,\n &#39;those&#39;,\n &#39;through&#39;,\n &#39;to&#39;,\n &#39;too&#39;,\n &#39;under&#39;,\n &#39;until&#39;,\n &#39;up&#39;,\n &#39;ve&#39;,\n &#39;very&#39;,\n &#39;was&#39;,\n &#39;wasn&#39;,\n &#34;wasn&#39;t&#34;,\n &#39;we&#39;,\n &#39;were&#39;,\n &#39;weren&#39;,\n &#34;weren&#39;t&#34;,\n &#39;what&#39;,\n &#39;when&#39;,\n &#39;where&#39;,\n &#39;which&#39;,\n &#39;while&#39;,\n &#39;who&#39;,\n &#39;whom&#39;,\n &#39;why&#39;,\n &#39;will&#39;,\n &#39;with&#39;,\n &#39;won&#39;,\n &#34;won&#39;t&#34;,\n &#39;wouldn&#39;,\n &#34;wouldn&#39;t&#34;,\n &#39;y&#39;,\n &#39;you&#39;,\n &#34;you&#39;d&#34;,\n &#34;you&#39;ll&#34;,\n &#34;you&#39;re&#34;,\n &#34;you&#39;ve&#34;,\n &#39;your&#39;,\n &#39;yours&#39;,\n &#39;yourself&#39;,\n &#39;yourselves&#39;}</div>"]}}],"execution_count":17},{"cell_type":"code","source":["import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nmy_text = [\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\"]\n# Incorporate stop words when creating the count vectorizer\ncv = CountVectorizer(stop_words='english')\nX = cv.fit_transform(my_text)\npd.DataFrame(X.toarray(), columns=cv.get_feature_names())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>black</th>\n      <th>buy</th>\n      <th>cucumbers</th>\n      <th>eyed</th>\n      <th>going</th>\n      <th>hi</th>\n      <th>mr</th>\n      <th>peas</th>\n      <th>pick</th>\n      <th>smith</th>\n      <th>store</th>\n      <th>tomatoes</th>\n      <th>vegetables</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n\nmy_text = [\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\"]\n\ncv = CountVectorizer()\nX = cv.fit_transform(my_text)\npd.DataFrame(X.toarray(),columns = cv.get_feature_names())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>and</th>\n      <th>as</th>\n      <th>black</th>\n      <th>buy</th>\n      <th>cucumbers</th>\n      <th>eyed</th>\n      <th>from</th>\n      <th>going</th>\n      <th>hi</th>\n      <th>mr</th>\n      <th>peas</th>\n      <th>pick</th>\n      <th>should</th>\n      <th>smith</th>\n      <th>some</th>\n      <th>store</th>\n      <th>the</th>\n      <th>to</th>\n      <th>tomatoes</th>\n      <th>up</th>\n      <th>vegetables</th>\n      <th>well</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["from nltk.stem.lancaster import LancasterStemmer\n\nstemmer = LancasterStemmer()\nprint('drive: {}'.format(stemmer.stem('drive')))\nprint('drive: {}'.format(stemmer.stem('driver')))\nprint('drive: {}'.format(stemmer.stem('drives')))\nprint('drive: {}'.format(stemmer.stem('driving')))\nprint('drive: {}'.format(stemmer.stem('driven')))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">drive: driv\ndrive: driv\ndrive: driv\ndrive: driv\ndrive: driv\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["## Preprocessing: Parts of Speech Tagging"],"metadata":{}},{"cell_type":"code","source":["from nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\nnltk.download('averaged_perceptron_tagger')\nmy_text= \"James Smith lives in the United States.\"\ntoken =  pos_tag(word_tokenize(my_text))\nprint(token)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[(&#39;James&#39;, &#39;NNP&#39;), (&#39;Smith&#39;, &#39;NNP&#39;), (&#39;lives&#39;, &#39;VBZ&#39;), (&#39;in&#39;, &#39;IN&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;United&#39;, &#39;NNP&#39;), (&#39;States&#39;, &#39;NNPS&#39;), (&#39;.&#39;, &#39;.&#39;)]\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["## Preprocessing: Named Entity Recognition(NER Recognition)"],"metadata":{}},{"cell_type":"code","source":["from nltk.chunk import ne_chunk\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\ntoken = pos_tag(word_tokenize(my_text))\nentities = ne_chunk(token)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n[nltk_data] Downloading package words to /root/nltk_data...\n[nltk_data]   Unzipping corpora/words.zip.\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["md ## Code: Compound Term Extraction"],"metadata":{}},{"cell_type":"code","source":["from nltk.tokenize import MWETokenizer\nmy_text = \"You all are the greatest students of all time.\"\nMwe_tokenizer = MWETokenizer([('You','all'),('of','all','time')])\nMWE_token =  Mwe_tokenizer.tokenize(word_tokenize(my_text))\nprint(MWE_token)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;You_all&#39;, &#39;are&#39;, &#39;the&#39;, &#39;greatest&#39;, &#39;students&#39;, &#39;of_all_time&#39;, &#39;.&#39;]\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"NLTK_practice","notebookId":4323332839545097},"nbformat":4,"nbformat_minor":0}
